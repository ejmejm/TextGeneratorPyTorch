{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drew on some of this work: https://www.kaggle.com/vsmolyakov/keras-cnn-with-fasttext-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import os, re, csv, math, codecs\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers, optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyt_df = pd.read_csv('../data/nyt_ts_headline.csv', index_col=0)\n",
    "breitbart_df = pd.read_csv('../data/breitbart_ts_headline.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01T00:00:00Z</th>\n",
       "      <td>The Week on Instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01T22:00:56Z</th>\n",
       "      <td>Mass Master</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01T21:17:09Z</th>\n",
       "      <td>Friday Night Music: More Wild Reeds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01T00:00:00Z</th>\n",
       "      <td>Our Favorite Styles Photos of 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01T19:10:36Z</th>\n",
       "      <td>Wishes for the New Year</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 headline\n",
       "2016-01-01T00:00:00Z                The Week on Instagram\n",
       "2016-01-01T22:00:56Z                          Mass Master\n",
       "2016-01-01T21:17:09Z  Friday Night Music: More Wild Reeds\n",
       "2016-01-01T00:00:00Z   Our Favorite Styles Photos of 2015\n",
       "2016-01-01T19:10:36Z              Wishes for the New Year"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 100000\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "676it [00:00, 6758.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999995it [01:51, 8951.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 999994 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('loading word embeddings...')\n",
    "embeddings_index = {}\n",
    "f = codecs.open('./wiki-news-300d-1M.vec', encoding='utf-8')\n",
    "i = 0\n",
    "for line in tqdm(f):\n",
    "    if i != 0:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    i += 1\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_df['y'] = [[1, 0]] * len(nyt_df)\n",
    "breitbart_df['y'] = [[0, 1]] * len(breitbart_df)\n",
    "df_all = pd.concat([nyt_df, breitbart_df], ignore_index=True)\n",
    "df_all = df_all.sample(frac=1., random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['cleaned'] = df_all['headline'].apply(lambda x: \n",
    "                            ' '.join([word for word in tokenizer.tokenize(x) \n",
    "                             if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>y</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>224829</th>\n",
       "      <td>Chicago Cubs Award Controversial Fan Steve Bar...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>Chicago Cubs Award Controversial Fan Steve Bar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296055</th>\n",
       "      <td>Carpetbagging: Norman Lear, Hollywood Producer...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>Carpetbagging Norman Lear Hollywood Producer W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260125</th>\n",
       "      <td>Donald Trump: Missing FBI Texts ‘One of the Bi...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>Donald Trump Missing FBI Texts One Biggest Sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12328</th>\n",
       "      <td>Classical Music Listings for Feb. 19-25</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>Classical Music Listings Feb 19 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261893</th>\n",
       "      <td>Germany Revealed as Biggest EU Rule Breaker as...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>Germany Revealed Biggest EU Rule Breaker Bloc ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 headline       y  \\\n",
       "224829  Chicago Cubs Award Controversial Fan Steve Bar...  [0, 1]   \n",
       "296055  Carpetbagging: Norman Lear, Hollywood Producer...  [0, 1]   \n",
       "260125  Donald Trump: Missing FBI Texts ‘One of the Bi...  [0, 1]   \n",
       "12328             Classical Music Listings for Feb. 19-25  [1, 0]   \n",
       "261893  Germany Revealed as Biggest EU Rule Breaker as...  [0, 1]   \n",
       "\n",
       "                                                  cleaned  \n",
       "224829  Chicago Cubs Award Controversial Fan Steve Bar...  \n",
       "296055  Carpetbagging Norman Lear Hollywood Producer W...  \n",
       "260125  Donald Trump Missing FBI Texts One Biggest Sto...  \n",
       "12328                  Classical Music Listings Feb 19 25  \n",
       "261893  Germany Revealed Biggest EU Rule Breaker Bloc ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Donald Trump: Missing FBI Texts ‘One of the Biggest Stories in a Long Time’'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.iloc[2]['headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict size: 63746\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 20\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(df_all['cleaned'])\n",
    "\n",
    "df_all['x'] = tokenizer.texts_to_sequences(df_all['cleaned'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('dict size:', len(word_index))\n",
    "\n",
    "df_all['x'] = [np.array(x) for x in sequence.pad_sequences(df_all['x'].values, maxlen=max_seq_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "n_epochs = 10\n",
    "\n",
    "n_filters = 64\n",
    "embed_dim = 300\n",
    "weight_decay = 1e-4\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "number of null word embeddings: 18713\n"
     ]
    }
   ],
   "source": [
    "#embedding matrix\n",
    "print('preparing embedding matrix...')\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample words not found:  ['nitka' 'strampel' 'zamata' 'wojcicki' 'welty' 'angulo' 'wolfson'\n",
      " 'fybish' 'miazga' 'kusama']\n"
     ]
    }
   ],
   "source": [
    "print(\"sample words not found: \", np.random.choice(words_not_found, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_split = 0.2\n",
    "\n",
    "n_test = int(len(df_all) * test_split)\n",
    "\n",
    "x_vals = np.stack(df_all['x'].values)\n",
    "y_vals = np.stack(df_all['y'].values)\n",
    "\n",
    "train_X, test_X = x_vals[:-n_test], x_vals[-n_test:]\n",
    "train_y, test_y = y_vals[:-n_test], y_vals[-n_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 300)           19123800  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 20, 64)            134464    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 10, 64)            28736     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 19,289,146\n",
      "Trainable params: 165,346\n",
      "Non-trainable params: 19,123,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#CNN architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, embed_dim,\n",
    "          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n",
    "model.add(Conv1D(n_filters, 7, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(n_filters, 7, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))  #multi-label (k-hot encoding)\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 217482 samples, validate on 24165 samples\n",
      "Epoch 1/10\n",
      "217482/217482 [==============================] - 56s 257us/step - loss: 0.2982 - acc: 0.8738 - val_loss: 0.2188 - val_acc: 0.9087\n",
      "Epoch 2/10\n",
      "217482/217482 [==============================] - 57s 261us/step - loss: 0.1904 - acc: 0.9262 - val_loss: 0.1833 - val_acc: 0.9263\n",
      "Epoch 3/10\n",
      "217482/217482 [==============================] - 55s 253us/step - loss: 0.1394 - acc: 0.9491 - val_loss: 0.1771 - val_acc: 0.9347\n",
      "Epoch 4/10\n",
      "217482/217482 [==============================] - 55s 251us/step - loss: 0.1059 - acc: 0.9624 - val_loss: 0.1852 - val_acc: 0.9337\n",
      "Epoch 5/10\n",
      "217482/217482 [==============================] - 55s 253us/step - loss: 0.0824 - acc: 0.9706 - val_loss: 0.2006 - val_acc: 0.9355\n",
      "Epoch 6/10\n",
      "217482/217482 [==============================] - 58s 264us/step - loss: 0.0650 - acc: 0.9773 - val_loss: 0.2203 - val_acc: 0.9335\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(train_X, train_y, \n",
    "                 batch_size=batch_size, \n",
    "                 epochs=n_epochs, \n",
    "                 callbacks=callbacks_list, \n",
    "                 validation_split=0.1, \n",
    "                 shuffle=True, \n",
    "                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 2s 101us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22394629516601564, 0.931125]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_X[:20000], test_y[:20000], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tokenizer', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
